# Reformer: The Efficient Transformer

Nikita Kitaev, Lukasz Kaiser, Anselm Levskaya.

## Summary

- one-line summary
- topic words : 
- base model : 
- variation : 
- benefits :
- weakness :
- future works :

## Abstract

Large Transformer은 다양한 분야에서 SOTA를 이뤘지만 특히 긴 시퀸스에 대해 굉장히 무거운 모델임. 이 논문에서는 이를 효율적으로 다루기 위한 방법을 이야기하며, 예로 dot-product대신에 locality-sensitive attention을 쓸 것임. 또한 reversible residual를 이용해서 activation을 한번만 이용할 수 있게 함. 그리고 이를 reformer라 명명.

## 1. Introduction



## 2. Related works

## 3. Model description

## 4. Experiment

## 5. Future works

## 6. Citation
